import openai
import feedparser
import requests
import os
from openai import OpenAI
from urllib.parse import quote


# ====== é…ç½® GPT æ¥å£ ======

client = OpenAI(
    api_key="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",
    base_url="https://openrouter.ai/api/v1"
)

# ====== æå–å…³é”®è¯ ======
def extract_keywords(user_input):
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ä»ç”¨æˆ·è¾“å…¥çš„ä¸€æ®µè¯ä¸­æå–3~5ä¸ªç”¨äºarXivæœç´¢çš„è‹±æ–‡å…³é”®è¯ã€‚"
        "è¾“å‡ºæ ¼å¼ä¸ºJSONæ•°ç»„ï¼Œå¦‚ï¼š[\"keyword1\", \"keyword2\"]"
    )

    response = client.chat.completions.create(
        model="openai/chatgpt-4o-latest",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ],
        extra_headers={
            "HTTP-Referer": "https://your-site.com",  # å¯é€‰ï¼Œç”¨äº OpenRouter ç»Ÿè®¡
            "X-Title": "ArxivQueryTool"              # å¯é€‰ï¼Œé¡¹ç›®åç§°
        }
    )

    content = response.choices[0].message.content.strip()

    try:
        import json
        keywords = json.loads(content)
        return keywords
    except Exception as e:
        print(f"âŒ è§£æå…³é”®è¯å¤±è´¥ï¼Œè¿”å›å†…å®¹ä¸ºï¼š{content}")
        raise e


# ====== æœç´¢ arXiv å¹¶ä¸‹è½½ PDF ======
def search_arxiv_and_download(keywords, max_results=5, download_dir="arxiv_papers"):
    os.makedirs(download_dir, exist_ok=True)

    for keyword in keywords:
        print(f"\nğŸ” Searching for: {keyword}")
        base_url = "http://export.arxiv.org/api/query?"
        encoded_keyword = quote(keyword)
        query_url = f"{base_url}search_query=all:{encoded_keyword}&start=0&max_results={max_results}"
        
        feed = feedparser.parse(query_url)
        if not feed.entries:
            print(f"âŒ No results for: {keyword}")
            continue

        for i, entry in enumerate(feed.entries):
            title = entry.title
            authors = ', '.join(author.name for author in entry.authors)
            summary = entry.summary.strip().replace("\n", " ")
            published = entry.published
            pdf_url = next((l.href for l in entry.links if l.type == "application/pdf"), None)

            print(f"\nğŸ“„ Paper {i+1}")
            print(f"Title: {title}")
            print(f"Authors: {authors}")
            print(f"Published: {published}")
            print(f"PDF Link: {pdf_url}")

            # ä¸‹è½½ PDF
            if pdf_url:
                try:
                    pdf_response = requests.get(pdf_url)
                    safe_title = "".join(c for c in title if c.isalnum() or c in " _-").rstrip()
                    pdf_path = os.path.join(download_dir, f"{safe_title[:100]}.pdf")
                    with open(pdf_path, "wb") as f:
                        f.write(pdf_response.content)
                    print(f"ğŸ“¥ Saved to: {pdf_path}")
                except Exception as e:
                    print(f"âš ï¸ Error downloading PDF: {e}")


# ====== ä¸»ç¨‹åºå…¥å£ ======
def main():
    print("ğŸ¯ æ¬¢è¿ä½¿ç”¨ arXiv æ™ºèƒ½æœç´¢åŠ©æ‰‹")
    user_sentence = input("è¯·è¾“å…¥ä½ æƒ³æŸ¥è¯¢çš„ä¸€æ®µè¯ï¼š\n> ")
    
    try:
        keywords = extract_keywords(user_sentence)
    except Exception as e:
        print(f"âŒ å…³é”®è¯æå–å¤±è´¥ï¼š{e}")
        return

    print("\nğŸ§  GPTæ¨èçš„å…³é”®è¯å¦‚ä¸‹ï¼š")
    for i, kw in enumerate(keywords):
        print(f"{i+1}. {kw}")

    selected = input("\nè¯·è¾“å…¥ä½ æƒ³ä½¿ç”¨çš„å…³é”®è¯ç¼–å·ï¼ˆå¯å¤šé€‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚1,3ï¼‰ï¼š\n> ")
    try:
        chosen_keywords = [keywords[int(i.strip()) - 1] for i in selected.split(",")]
    except:
        print("âŒ è¾“å…¥æ ¼å¼æœ‰è¯¯ï¼Œè¯·è¾“å…¥æœ‰æ•ˆç¼–å·ã€‚")
        return

    search_arxiv_and_download(chosen_keywords, max_results=5)


# ====== å¯åŠ¨ ======
if __name__ == "__main__":
    main()
